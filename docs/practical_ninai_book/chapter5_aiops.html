<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Chapter 5 — Ninai for AIOps</title>
  <link rel="stylesheet" href="assets/book.css">
</head>
<body>
  <header class="book">
    <div class="kicker">Practical Ninai - Chapter 5</div>
    <h1>Ninai for AIOps Consultants</h1>
    <p class="lead">AIOps is where agentic systems prove their value—or fail loudly. This chapter focuses on safe automation, auditability, and measurable outcomes: alert triage, diagnostics, runbooks, and postmortem-quality writebacks.</p>
  </header>

  <nav class="toc no-print" aria-label="Chapter contents">
    <strong>In this chapter</strong>
    <ul>
      <li><a href="#positioning">Positioning: what Ninai does in AIOps</a></li>
      <li><a href="#loop">The incident-to-memory loop</a></li>
      <li><a href="#use-cases">Core use cases</a></li>
      <li><a href="#runbooks">Runbooks and automation gates</a></li>
      <li><a href="#memory-model">Incident memory model</a></li>
      <li><a href="#recipes">Practical recipes</a></li>
      <li><a href="#metrics">Metrics and SLOs</a></li>
      <li><a href="#rollout">Rollout plan (consulting playbook)</a></li>
    </ul>
  </nav>

  <h2 id="positioning">Positioning: what Ninai does in AIOps</h2>
  <p>Ninai is best thought of as the shared brain for incident context:</p>
  <ul>
    <li><strong>Memory</strong>: persistent incident summaries, diagnostics, and validated runbooks.</li>
    <li><strong>Policy</strong>: deterministic gates for automation (approve, deny, or allow).</li>
    <li><strong>Evidence</strong>: links to logs, dashboards, tickets, and commands executed.</li>
  </ul>

  <div class="callout note">
    <div class="title">AIOps truth</div>
    <p>The most valuable automation is not “restart everything”. It is reducing cognitive load: the right context, the right runbook, the right evidence, and a safe path to action.</p>
  </div>

  <h2 id="loop">The incident-to-memory loop</h2>
  <figure>
    <img src="assets/diagrams/aiops_incident_to_memory_loop.svg" alt="AIOps incident to memory loop">
    <figcaption>Figure 5-1. Incidents become durable memories that prevent future incidents.</figcaption>
  </figure>

  <h2 id="use-cases">Core use cases</h2>
  <h3>Use case 1: alert triage and contextualization</h3>
  <p>Given an alert, Ninai can assemble a context packet: relevant dashboards, recent deploys, prior incidents with similar signatures, and a short hypothesis list. The critical requirement is evidence: every claim should map to a log line, metric, or validated memory.</p>

  <h3>Use case 2: diagnostics automation (read-only)</h3>
  <p>Most organizations can safely automate read-only diagnostics early: fetch logs, run queries, compare with baselines, and attach artifacts to the incident memory. This is high leverage because it happens at the worst time: under pressure.</p>

  <h3>Use case 3: runbook execution with gates</h3>
  <p>Automated remediation is where risk lives. Use explicit policy rules: approvals, blast-radius limits, and reversibility requirements.</p>

  <h3>Use case 4: postmortem and prevention</h3>
  <p>After an incident, write back the summary, the evidence, and what worked. Promote validated runbooks to durable procedural memory after review.</p>

  <h2 id="runbooks">Runbooks and automation gates</h2>
  <p>Runbooks are procedural knowledge. Treat them like code: versioned, reviewed, and tested in staging.</p>

  <h3>Gate 1: read-only vs mutating actions</h3>
  <ul>
    <li><strong>Read-only tools</strong>: safe to run automatically (fetch logs, list pods, query metrics).</li>
    <li><strong>Mutating tools</strong>: require approvals by default (restart services, rollbacks, scaling changes).</li>
  </ul>

  <h3>Gate 2: blast radius limits</h3>
  <p>Enforce hard limits on automation, for example:</p>
  <ul>
    <li>Never restart more than one service at a time.</li>
    <li>Never apply changes across regions without explicit approval.</li>
    <li>Never touch production if staging has not been tested recently.</li>
  </ul>

  <div class="callout warn">
    <div class="title">Avoid “implicit approvals”</div>
    <p>If the system cannot identify an owner, or a policy condition fails, it should not proceed. Silent automation drift is a common failure mode.</p>
  </div>

  <h2 id="memory-model">Incident memory model</h2>
  <p>A consistent incident memory model is the foundation for retrieval and prevention. Keep it boring and repeatable.</p>
  <ul>
    <li><strong>Summary</strong>: what happened and impact (tags: <code>incident</code>, <code>summary</code>)</li>
    <li><strong>Evidence</strong>: links to metrics/logs/traces and attachments</li>
    <li><strong>Decisions</strong>: what was tried, what succeeded, what failed</li>
    <li><strong>Root cause</strong>: when validated (tags: <code>rca</code>, <code>validated</code>)</li>
    <li><strong>Runbook</strong>: procedural steps that were effective (type: <code>procedural</code>)</li>
  </ul>

  <h2 id="recipes">Practical recipes</h2>
  <h3>Recipe: store an incident summary</h3>
  <pre><code>from ninai import NinaiClient

client = NinaiClient(api_key="nai_your_key", organization_id="org_platform")

incident_id = "INC-88201"
client.memories.create(
    title=f"Incident summary - {incident_id}",
    content=(
        "Impact: elevated 500s for checkout for 12 minutes. "
        "Suspected cause: dependency timeout spike after deploy 2026.01.24.3."
    ),
    tags=["incident", "summary", "checkout"],
    entities={"incident_id": incident_id, "service": "checkout", "severity": "sev2"},
    scope="organization",
    classification="internal",
)</code></pre>

  <h3>Recipe: retrieve similar incidents</h3>
  <pre><code>similar = client.memories.search(
    query="checkout 500s dependency timeout after deploy",
    scope="organization",
    tags=["incident"],
    limit=8,
)

for item in similar.items:
    print(item.title, item.score)</code></pre>

  <h3>Recipe: promote a runbook after review</h3>
  <p>Keep drafts narrow (personal/team). Promote to organizational procedural memory only after review:</p>
  <pre><code>client.memories.create(
    title="Runbook: mitigate checkout dependency timeouts",
    content=(
        "1) Confirm timeout spike in dependency metrics.\n"
        "2) Roll back last deploy if correlation is strong.\n"
        "3) Increase timeout only if rollback unavailable.\n"
        "4) Validate error rate returns to baseline.\n"
        "5) Open follow-up for root cause analysis."
    ),
    tags=["runbook", "validated", "checkout"],
    entities={"service": "checkout"},
    scope="organization",
    classification="internal",
    memory_type="procedural",
)</code></pre>

  <h2 id="metrics">Metrics and SLOs</h2>
  <p>AIOps should be measured like any other system. A few practical metrics that work across teams:</p>
  <ul>
    <li><strong>MTTA / MTTD / MTTR</strong>: time to acknowledge/detect/resolve</li>
    <li><strong>Automation assist rate</strong>: % incidents where Ninai provided useful context or a correct runbook</li>
    <li><strong>Unsafe action blocks</strong>: number of times policy prevented a risky action</li>
    <li><strong>Rollback success rate</strong>: if you automate rollbacks, measure correctness</li>
    <li><strong>Postmortem completeness</strong>: incidents with evidence links and validated root cause</li>
  </ul>

  <div class="callout tip">
    <div class="title">SLOs for your Ninai deployment</div>
    <p>Define SLOs for retrieval latency and search quality (for example: “top-5 similarity contains at least one relevant prior incident for 70% of sev2 incidents”).</p>
  </div>

  <h2 id="rollout">Rollout plan (consulting playbook)</h2>
  <ol>
    <li><strong>Week 1: read-only mode</strong> — ingest incidents and build search; no automation.</li>
    <li><strong>Week 2: diagnostics</strong> — automate safe data collection; attach artifacts; standardize summaries.</li>
    <li><strong>Week 3: approvals</strong> — add approval workflows for a small set of runbooks.</li>
    <li><strong>Week 4: limited remediation</strong> — allow only reversible actions with strict blast-radius limits.</li>
    <li><strong>Ongoing: governance</strong> — promote validated runbooks; retire obsolete ones; run regression tests.</li>
  </ol>

  <div class="footer">
    <p>Appendix: See the gaps list for missing implementations assumed by some advanced automation examples.</p>
  </div>
</body>
</html>
