<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Chapter 4 - Architects: Design Patterns, Infra, and Scale</title>
  <link rel="stylesheet" href="assets/book.css">
</head>
<body>
  <header class="book">
    <div class="kicker">Practical Ninai - Chapter 4</div>
    <h1>Architects: Design Patterns, Infra, and Scale</h1>
    <p class="lead">Ninai can be demoed as a single process, but production Ninai is a platform: tenancy boundaries, policy enforcement, repeatable runs, safe tool execution, and operational discipline. This chapter gives you reference architectures, patterns, and practical checklists.</p>
  </header>

  <nav class="toc no-print" aria-label="Chapter contents">
    <strong>In this chapter</strong>
    <ul>
      <li><a href="#mental-model">A useful mental model</a></li>
      <li><a href="#principles">Architecture principles</a></li>
      <li><a href="#core-arch">Core runtime architecture</a></li>
      <li><a href="#execution">Execution model: runs, workers, and queues</a></li>
      <li><a href="#tool-boundary">Tool boundary: sandboxing, idempotency, approvals</a></li>
      <li><a href="#memory">Memory at scale: TTL, promotion, governance</a></li>
      <li><a href="#tenancy">Tenancy: identity, RBAC, and RLS</a></li>
      <li><a href="#observability">Observability and auditability</a></li>
      <li><a href="#infra">Cloud infrastructure reference design</a></li>
      <li><a href="#capacity">Capacity planning: what to measure and why</a></li>
      <li><a href="#cases">Case studies</a></li>
      <li><a href="#checklists">Architect checklists</a></li>
    </ul>
  </nav>

  <h2 id="mental-model">A useful mental model</h2>
  <p>Think of Ninai as a safe automation platform that happens to use LLMs for planning and language. The model is not the system of record; your databases and event logs are. The model should never be trusted to bypass policy. Your platform should still be safe if the model output is wrong or adversarial.</p>

  <div class="callout note">
    <div class="title">Three truths that keep you out of trouble</div>
    <ul>
      <li>All side effects must go through a tool boundary.</li>
      <li>All reads and writes must be tenant-scoped (and enforced by RLS).</li>
      <li>Everything important must be replayable and auditable.</li>
    </ul>
  </div>

  <h2 id="principles">Architecture principles</h2>
  <h3>Principle 1: Decompose into primitives</h3>
  <p>Agents are orchestrators, not monoliths. Keep the primitives separate so you can test and scale them independently:</p>
  <ul>
    <li><strong>Goals</strong>: goal graphs, nodes, dependencies, evidence links, and lifecycle state.</li>
    <li><strong>Memory</strong>: retrieval, storage, promotion, and retention policies.</li>
    <li><strong>Tools</strong>: connectors with explicit schemas, timeouts, retries, and audit records.</li>
    <li><strong>Policy</strong>: deterministic allow/deny/approve decisions enforced in code.</li>
    <li><strong>Supervision</strong>: risk-based review, escalation, and approval workflows.</li>
  </ul>

  <h3>Principle 2: Fail-closed default posture</h3>
  <p>Fail-closed means: when uncertain, the system stops and asks. It is the correct default for banking, AIOps remediation, enterprise security, and any environment where the cost of being wrong is high.</p>

  <h3>Principle 3: Design for the two loops</h3>
  <p>Every production agent system has two loops:</p>
  <ul>
    <li><strong>Execution loop</strong>: plan -> tool calls -> evidence -> outcome.</li>
    <li><strong>Learning loop</strong>: what worked, what failed, what is reliable, what requires approval, what should be promoted to durable knowledge.</li>
  </ul>
  <p>Ninai features like self-model signals, meta-supervision, and simulation exist to make the learning loop safe and controlled.</p>

  <h2 id="core-arch">Core runtime architecture</h2>
  <figure>
    <img src="assets/diagrams/ninai_core_architecture.svg" alt="Ninai core runtime architecture">
    <figcaption>Figure 4-1. Agents orchestrate primitives; observability and audit are first-class.</figcaption>
  </figure>

  <h3>What belongs in the API tier</h3>
  <ul>
    <li>Authentication and request signing (API keys, JWTs, mTLS depending on your environment)</li>
    <li>Tenant routing and tenancy context propagation</li>
    <li>Rate limiting, quotas, and backpressure signals</li>
    <li>Fast CRUD endpoints for goals, memory metadata, and run status</li>
  </ul>

  <h3>What belongs in worker tier</h3>
  <ul>
    <li>LLM planning and structured output validation</li>
    <li>Memory retrieval and evidence linking</li>
    <li>Tool orchestration through the tool boundary</li>
    <li>Meta-review and escalation decisions</li>
  </ul>

  <div class="callout warn">
    <div class="title">Architectural anti-pattern</div>
    <p>Putting long-running agent work (LLM calls, tool calls, retries) inside a single HTTP request will eventually collapse under load. You want run IDs, queues, and background workers.</p>
  </div>

  <h2 id="execution">Execution model: runs, workers, and queues</h2>
  <p>The most stable production model is: clients create a run, the platform processes it asynchronously, and clients subscribe to run events. This is how you achieve reliability, replay, and cost control.</p>

  <figure>
    <img src="assets/diagrams/ninai_goal_to_tools_flow.svg" alt="Goal to tools flow">
    <figcaption>Figure 4-2. Request to goal to policy to tools to evidence to supervision.</figcaption>
  </figure>

  <h3>Run records</h3>
  <p>A run should have an immutable timeline: inputs, decisions, tool calls, and outputs. Architecturally, this is your debugging superpower.</p>

  <h3>Queues: your load shedder</h3>
  <p>Queues are not just for throughput. They are how you prevent a thundering herd from taking down the system. You can scale workers based on queue depth and processing time.</p>

  <h2 id="tool-boundary">Tool boundary: sandboxing, idempotency, approvals</h2>
  <p>Tools are where the world changes. Treat tools like a "kernel boundary" for the agent: strict input validation, explicit permissions, and safe failure semantics.</p>

  <h3>Idempotency and retries</h3>
  <ul>
    <li>Require an idempotency key for any tool that mutates state.</li>
    <li>Make retry behavior explicit per tool (some tools are safe to retry, some are not).</li>
    <li>Persist the tool call record before execution so you can deduplicate.</li>
  </ul>

  <h3>Approvals for high-risk actions</h3>
  <p>Approvals should be a first-class workflow. Examples: "restart production", "freeze account", "email customer segment", "publish knowledge base article".</p>
  <div class="callout tip">
    <div class="title">Practical policy rule</div>
    <p>Separate tools into read-only tools (low risk) and mutating tools (higher risk). Default mutating tools to approval-required unless a tenant explicitly opts into automation for that tool.</p>
  </div>

  <h2 id="memory">Memory at scale: TTL, promotion, governance</h2>
  <p>Memory is both cost and risk. Architect memory with clear lifecycle rules:</p>
  <ul>
    <li><strong>Short-term memory</strong>: per-run context, expires automatically.</li>
    <li><strong>Long-term memory</strong>: durable knowledge, requires promotion and governance.</li>
    <li><strong>Evidence links</strong>: pointers from goals/runs to memory items and artifacts.</li>
  </ul>

  <h3>Promotion rules</h3>
  <p>Promotion should require evidence and risk checks. Example checks:</p>
  <ul>
    <li>Contains sensitive keywords (secrets, credentials)</li>
    <li>Classification requires restricted access</li>
    <li>Too short / low value content should not become long-term</li>
  </ul>

  <h2 id="tenancy">Tenancy: identity, RBAC, and RLS</h2>
  <p>Multi-tenant systems fail when tenancy is optional. In Ninai, tenancy should be enforced at every boundary:</p>
  <ul>
    <li>API request includes org/user identity.</li>
    <li>Tenant context is set on DB sessions.</li>
    <li>RLS policies enforce tenant isolation even if application code is wrong.</li>
    <li>Memory retrieval is filtered by tenant and clearance.</li>
  </ul>

  <div class="callout note">
    <div class="title">RLS as last line of defense</div>
    <p>Put <code>tenant_id</code> (or org_id) on every table. Enable RLS. In tests, validate that cross-org reads are impossible.</p>
  </div>

  <h2 id="observability">Observability and auditability</h2>
  <p>You need to observe three things:</p>
  <ol>
    <li><strong>System health</strong>: latency, error rate, saturation, queue depth.</li>
    <li><strong>Agent quality</strong>: tool success rate, policy blocks, escalations, human overrides.</li>
    <li><strong>Compliance</strong>: who did what, what evidence was used, why it was allowed.</li>
  </ol>

  <h3>Minimum audit record for tool calls</h3>
  <ul>
    <li>tenant_id, run_id, actor identity</li>
    <li>tool name, normalized params (or hash), idempotency key</li>
    <li>policy decision and justification</li>
    <li>result summary and artifact pointers</li>
  </ul>

  <h2 id="infra">Cloud infrastructure reference design</h2>
  <p>Start cloud-agnostic, then map to AWS/Azure/GCP services. Avoid provider-specific icons so the diagram is portable and safe to embed.</p>

  <figure>
    <img src="assets/diagrams/ninai_infra_reference.svg" alt="Ninai cloud-agnostic infrastructure reference">
    <figcaption>Figure 4-3. Reference infra: Kubernetes plus managed data plane.</figcaption>
  </figure>

  <h3>Provider mappings</h3>
  <ul>
    <li><strong>AWS</strong>: EKS, RDS Postgres, SQS/MSK, S3, Secrets Manager/KMS, CloudWatch + OTel</li>
    <li><strong>Azure</strong>: AKS, Azure Postgres, Service Bus/Event Hubs, Blob Storage, Key Vault, Monitor + OTel</li>
    <li><strong>GCP</strong>: GKE, Cloud SQL, Pub/Sub, Cloud Storage, Secret Manager/KMS, Cloud Monitoring + OTel</li>
  </ul>

  <h2 id="capacity">Capacity planning: what to measure and why</h2>
  <p>Capacity planning should start with runs-in-flight and tool latency, not just HTTP traffic. A practical approximation:</p>
  <pre><code>concurrent_llm_calls  ~ (runs_per_minute * avg_llm_calls_per_run * avg_llm_latency_seconds) / 60
concurrent_tool_calls ~ (runs_per_minute * avg_tool_calls_per_run * avg_tool_latency_seconds) / 60</code></pre>

  <div class="callout warn">
    <div class="title">If tools are slow, you are tool-latency bound</div>
    <p>When external tools are slow (cloud APIs, ticketing systems), your system needs strict timeouts, circuit breakers, and queue-based execution. Otherwise you will accumulate stuck runs and amplify incidents.</p>
  </div>

  <h2 id="cases">Case studies</h2>

  <h3>Retail: bundle recommendations with approvals</h3>
  <p><strong>Challenge</strong>: a junior dev prototype sends promotions directly and causes wrong discounts.</p>
  <p><strong>Solution</strong>: separate planning from action, require approval for send-email tool, enforce segment constraints, and store evidence for all recommendations.</p>

  <h3>Banking: fraud triage with least privilege</h3>
  <p><strong>Challenge</strong>: the business wants automatic account freezes.</p>
  <p><strong>Solution</strong>: allow investigation tools automatically, require approval for freeze tool, enforce RLS, and persist the policy decision record for audit.</p>

  <h3>NASA-like: mission planning with phase gates</h3>
  <p><strong>Challenge</strong>: mission transitions require high assurance.</p>
  <p><strong>Solution</strong>: represent mission phases as goal nodes, require meta-supervision for phase transitions, and stage production actions after simulation.</p>

  <h3>Customer support: evidence-based answers</h3>
  <p><strong>Challenge</strong>: hallucinated answers are unacceptable.</p>
  <p><strong>Solution</strong>: enforce answer-only-with-evidence policy, attach KB articles as evidence links, and measure reopened-ticket rate as a quality metric.</p>

  <h2 id="checklists">Architect checklists</h2>

  <h3>Deployment checklist</h3>
  <ul>
    <li>Tenant identity required end-to-end; deny on ambiguity.</li>
    <li>Workers stateless; runs evented and replayable.</li>
    <li>Tool gateway isolated with allowlists and egress controls.</li>
    <li>Datastores have backups and tested restore procedures.</li>
  </ul>

  <h3>Security checklist</h3>
  <ul>
    <li>RLS enabled and tested for tenant isolation.</li>
    <li>Secrets never logged; rotated; scoped to least privilege.</li>
    <li>Approval workflow exists for high-risk tool actions.</li>
    <li>Audit records are tamper-evident or append-only.</li>
  </ul>

  <h3>Ops checklist</h3>
  <ul>
    <li>Dashboards: queue depth, worker saturation, tool failures, policy blocks.</li>
    <li>SLOs per run type; error budgets tracked.</li>
    <li>Incident playbooks: stuck runs, tool outages, LLM degradation.</li>
  </ul>

  <div class="footer">
    <p>Next: Chapter 5 focuses on AIOps patterns and runbooks.</p>
  </div>
</body>
</html>
